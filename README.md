# ğŸ¤– LLaMA Chatbot â€“ Local AI Assistant

Welcome to **LLaMA Chatbot**, a local Flask-based conversational assistant powered by the **LLaMA model**. This chatbot runs completely offline and can respond to user prompts in a natural, human-like way.

> ğŸ”’ No API keys needed. Fully local. Privacy-first.

---

## ğŸ§  Features

- âœ… Powered by a fine-tuned **LLaMA model**
- ğŸ§© Built with **Transformers**, **PyTorch**, and **Flask**
- ğŸ¨ Custom frontend with responsive UI
- ğŸŒ Supports **local hosting** without internet
- ğŸ’¬ Chat UI with instant feedback

---

## ğŸ–¥ï¸ Demo

![Chatbot Demo Screenshot]![image](https://github.com/user-attachments/assets/b78c2244-3555-48ec-b192-16084eb18957)


---

## ğŸ“¦ Folder Structure
```
llama-chatbot/
â”œâ”€â”€ app1.py
â”œâ”€â”€ templates/
â”‚ â””â”€â”€ index.html
â”œâ”€â”€ static/
â”‚ â”œâ”€â”€ styles.css
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## ğŸš€ Getting Started

### 1. Clone the repo

```bash
git clone https://github.com/your-username/llama-chatbot.git
cd llama-chatbot
```
2. Create a virtual environment (optional but recommended)
```
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```
3. Install dependencies
```
pip install -r requirements.txt
```
4. Make sure your fine-tuned model is in the directory
Update model_path in app.py to your correct path, e.g.:
```
model_path = "trained_model_20250708_032008"
```
5. Run the app
```
python app1.py
```
Visit: http://localhost:5005

---

âš™ï¸ **Tech Stack**  
- Flask Backend API & Web Server  
- Transformers for model loading & text generation  
- PyTorch deep learning engine  
- HTML / CSS / JS Frontend

---

ğŸ§ª **Sample Prompt**  
**You**: What is quantum computing?  
**Bot**: Quantum computing is a type of computing that uses the principles of quantum mechanics, a branch of physics that describes the behavior of matter and energy at the smallest scales.....

---

â“ **FAQ**  
- **Q**: Can this run on CPU?  
  **A**: Yes, but slower. GPU (CUDA) is preferred for real-time response.

- **Q**: Do I need internet or Hugging Face keys?  
  **A**: No. The model is fully local. Just point `model_path` to your fine-tuned directory.

- **Q**: Can I swap the LLaMA model with another?  
  **A**: Yes! As long as itâ€™s compatible with `AutoModelForCausalLM`.

---

ğŸ“„ **License**  
MIT License â€“ You can use, modify, and distribute it for both personal and commercial purposes.

---

âœ¨ **Author**  
Made by **Jasleen Kaur Matharoo**  
GitHub: [@Jasleen-05](https://github.com/Jasleen-05)  
Email: [jasleen.matharoo@s.amity.edu](mailto:jasleen.matharoo@s.amity.edu)
